{
    "Clearml Pipeline from Tasks":{
        
        "description": "Clearml Pipeline from Tasks",
        "prefix": "clearml:pipeline_tasks",
        "body": [
                "from clearml import Task",
                "from clearml.automation import PipelineController",
                "def pre_execute_callback_example(a_pipeline, a_node, current_param_override):",
                "",
                "    print(",
                "        \"Cloning Task id={} with parameters: {}\".format(",
                "            a_node.base_task_id, current_param_override",
                "        )",
                "    )",
                "    return True",
                "",
                "def post_execute_callback_example(a_pipeline, a_node):",
                "    ",
                "    print(\"Completed Task id={}\".format(a_node.executed))",
                "    ",
                "    return",
                "pipe = PipelineController(",
                "name=\"---Pipeline-name---\", project=\"examples\", version=\"0.0.1\", add_pipeline_tags=False",
                ")",
                "pipe.add_parameter(",
                "    \"url\",",
                "    \"https://files.community.clear.ml/examples%252F.pipelines%252FPipeline%20demo/stage_data.8f17b6316ce442ce8904f6fccb1763de/artifacts/dataset/f6d08388e9bc44c86cab497ad31403c4.iris_dataset.pkl\",",
                "    \"dataset_url\"",
                ")",

                "pipe.set_default_execution_queue(\"default\")",

                "pipe.add_step(",
                "    name=\"stage_data\",",
                "    base_task_project=\"examples\",",
                "    base_task_name=\"Pipeline step 1 dataset artifact\",",
                "    parameter_override={\"General/dataset_url\": \"${pipeline.url}\"},",
                ")",

                "pipe.add_step(",
                "    name=\"stage_process\",",
                "    parents=[\"stage_data\"],",
                "    base_task_project=\"examples\",",
                "    base_task_name=\"Pipeline step 2 process dataset\",",
                "    parameter_override={",
                "        \"General/dataset_url\": \"${stage_data.artifacts.dataset.url}\",",
                "        \"General/test_size\": 0.25,",
                "    },",
                "    pre_execute_callback=pre_execute_callback_example,",
                "    post_execute_callback=post_execute_callback_example,",
                ")",
                "pipe.add_step(",
                "    name=\"stage_train\",",
                "    parents=[\"stage_process\"],",
                "    base_task_project=\"examples\",",
                "    base_task_name=\"Pipeline step 3 train model\",",
                "    parameter_override={\"General/dataset_task_id\": \"${stage_process.id}\"},",
                ")",

                "pipe.start()",
                ]
    },
    "Clearml Pipeline from Decorators":{
            
            "description": "Clearml Pipeline from Decorators",
            "prefix": "clearml:pipeline_decorators",
            "body": [
                "from clearml.automation.controller import PipelineDecorator",
                "from clearml import TaskTypes",
"",
"",
                "@PipelineDecorator.component(return_values=['data_frame'], cache=True, task_type=TaskTypes.data_processing)",
                "def step_one(pickle_data_url: str, extra: int = 43):",
                "    print('step_one')",
                "    # make sure we have scikit-learn for this step, we need it to use to unpickle the object",
                "    import sklearn  # noqa",
                "    import pickle",
                "    import pandas as pd",
                "    from clearml import StorageManager",
                "    local_iris_pkl = StorageManager.get_local_copy(remote_url=pickle_data_url)",
                "    with open(local_iris_pkl, 'rb') as f:",
                "        iris = pickle.load(f)",
                "    data_frame = pd.DataFrame(iris['data'], columns=iris['feature_names'])",
                "    data_frame.columns += ['target']",
                "    data_frame['target'] = iris['target']",
                "    return data_frame",

                "@PipelineDecorator.component(return_values=['processed_data'], cache=True, task_type=TaskTypes.data_processing)",
                "def step_two(data_frame, test_size=0.2, random_state=42):",
                "    print('step_two')",
                "    # make sure we have pandas for this step, we need it to use the data_frame",
                "    import pandas as pd  # noqa",
                "    from sklearn.model_selection import train_test_split",
                "    y = data_frame['target']",
                "    X = data_frame[(c for c in data_frame.columns if c != 'target')]",
                "    X_train, X_test, y_train, y_test = train_test_split(",
                "        X, y, test_size=test_size, random_state=random_state)",
"",
                "    return X_train, X_test, y_train, y_test",



                "@PipelineDecorator.component(return_values=['model'], cache=True, task_type=TaskTypes.training)",
                "def step_three(data):",
                "    print('step_three')",
                "    # make sure we have pandas for this step, we need it to use the data_frame",
                "    import pandas as pd  # noqa",
                "    from sklearn.linear_model import LogisticRegression",
                "    X_train, X_test, y_train, y_test = data",
                "    model = LogisticRegression(solver='liblinear', multi_class='auto')",
                "    model.fit(X_train, y_train)",
                "    return model",


                "@PipelineDecorator.pipeline(name='custom pipeline logic', project='examples', version='0.0.5')",
                "def executing_pipeline(pickle_url, mock_parameter='mock'):",
                "    print('pipeline args:', pickle_url, mock_parameter)",
"",
                "    print('launch step one')",
                "    data_frame = step_one(pickle_url)",
"",
                "    print('launch step two')",
                "    processed_data = step_two(data_frame)",
"",
                "    print('launch step three')",
                "    model = step_three(processed_data)",
                "    print('pipeline completed with model: {}'.format(model))",


                "if __name__ == '__main__':             ",
                "",
                "    #PipelineDecorator.run_locally()",
"",
                "    executing_pipeline(",
                "        pickle_url='https://github.com/allegroai/events/raw/master/odsc20-east/generic/iris_dataset.pkl',",
                "    )",
                    
                    ]
        },
    }
